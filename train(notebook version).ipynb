{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN0RIXKQVNW+kiShyyxWEYF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pvBGgXfXUgQI","executionInfo":{"status":"ok","timestamp":1744822176531,"user_tz":-330,"elapsed":3011657,"user":{"displayName":"ABISHEK N","userId":"02974916691807934605"}},"outputId":"79b1e846-0773-4cad-bde9-eb33ff67569b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Installing onnx...\n","Created/Verified directory: /content/drive/MyDrive/Colab Notebooks/Euro\n","Created/Verified directory: /content/drive/MyDrive/Colab Notebooks/Euro/results_pytorch_all\n","Created/Verified directory: /content/drive/MyDrive/Colab Notebooks/Euro/results_pytorch_all/stored_test_images\n","Created/Verified directory: /content/drive/MyDrive/Colab Notebooks/Euro/data\n","Write permission confirmed for /content/drive/MyDrive/Colab Notebooks/Euro/data\n","Using device: cuda\n","Dataset not found or empty at /content/drive/MyDrive/Colab Notebooks/Euro/data/eurosat. Attempting to download...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 94.3M/94.3M [00:00<00:00, 149MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Dataset downloaded to /content/drive/MyDrive/Colab Notebooks/Euro/data/eurosat\n","Dataset verified. Contents: ['EuroSAT.zip', '2750']\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n","100%|██████████| 20.5M/20.5M [00:00<00:00, 62.7MB/s]\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/30], Train Loss: 0.7751, Val Loss: 0.1944\n","Current Learning Rate: 0.000100\n","Epoch [2/30], Train Loss: 0.2097, Val Loss: 0.1247\n","Current Learning Rate: 0.000100\n","Epoch [3/30], Train Loss: 0.1434, Val Loss: 0.0976\n","Current Learning Rate: 0.000100\n","Epoch [4/30], Train Loss: 0.0956, Val Loss: 0.0961\n","Current Learning Rate: 0.000100\n","Epoch [5/30], Train Loss: 0.0732, Val Loss: 0.0855\n","Current Learning Rate: 0.000100\n","Epoch [6/30], Train Loss: 0.0576, Val Loss: 0.0776\n","Current Learning Rate: 0.000100\n","Epoch [7/30], Train Loss: 0.0488, Val Loss: 0.0806\n","Current Learning Rate: 0.000100\n","Epoch [8/30], Train Loss: 0.0432, Val Loss: 0.0781\n","Current Learning Rate: 0.000100\n","Epoch [9/30], Train Loss: 0.0390, Val Loss: 0.0788\n","Current Learning Rate: 0.000100\n","Epoch [10/30], Train Loss: 0.0322, Val Loss: 0.0778\n","Current Learning Rate: 0.000100\n","Epoch [11/30], Train Loss: 0.0294, Val Loss: 0.0875\n","Current Learning Rate: 0.000100\n","Epoch [12/30], Train Loss: 0.0261, Val Loss: 0.0819\n","Current Learning Rate: 0.000010\n","Epoch [13/30], Train Loss: 0.0196, Val Loss: 0.0754\n","Current Learning Rate: 0.000010\n","Epoch [14/30], Train Loss: 0.0161, Val Loss: 0.0772\n","Current Learning Rate: 0.000010\n","Epoch [15/30], Train Loss: 0.0183, Val Loss: 0.0771\n","Current Learning Rate: 0.000010\n","Epoch [16/30], Train Loss: 0.0166, Val Loss: 0.0819\n","Current Learning Rate: 0.000010\n","Epoch [17/30], Train Loss: 0.0135, Val Loss: 0.0718\n","Current Learning Rate: 0.000010\n","Epoch [18/30], Train Loss: 0.0142, Val Loss: 0.0771\n","Current Learning Rate: 0.000010\n","Epoch [19/30], Train Loss: 0.0122, Val Loss: 0.0717\n","Current Learning Rate: 0.000010\n","Epoch [20/30], Train Loss: 0.0101, Val Loss: 0.0809\n","Current Learning Rate: 0.000010\n","Epoch [21/30], Train Loss: 0.0113, Val Loss: 0.0782\n","Current Learning Rate: 0.000010\n","Epoch [22/30], Train Loss: 0.0117, Val Loss: 0.0743\n","Current Learning Rate: 0.000010\n","Epoch [23/30], Train Loss: 0.0104, Val Loss: 0.0730\n","Current Learning Rate: 0.000010\n","Epoch [24/30], Train Loss: 0.0110, Val Loss: 0.0741\n","Current Learning Rate: 0.000010\n","Epoch [25/30], Train Loss: 0.0120, Val Loss: 0.0717\n","Current Learning Rate: 0.000010\n","Epoch [26/30], Train Loss: 0.0074, Val Loss: 0.0781\n","Current Learning Rate: 0.000010\n","Epoch [27/30], Train Loss: 0.0098, Val Loss: 0.0809\n","Current Learning Rate: 0.000010\n","Epoch [28/30], Train Loss: 0.0109, Val Loss: 0.0787\n","Current Learning Rate: 0.000010\n","Epoch [29/30], Train Loss: 0.0093, Val Loss: 0.0802\n","Current Learning Rate: 0.000010\n","Epoch [30/30], Train Loss: 0.0098, Val Loss: 0.0747\n","Current Learning Rate: 0.000010\n","\n","Classification Report:\n","                      precision    recall  f1-score   support\n","\n","          AnnualCrop       0.98      0.97      0.98       472\n","              Forest       1.00      0.99      0.99       442\n","HerbaceousVegetation       0.96      0.97      0.97       458\n","             Highway       0.97      0.97      0.97       391\n","          Industrial       0.99      0.97      0.98       378\n","             Pasture       0.95      0.97      0.96       299\n","       PermanentCrop       0.98      0.96      0.97       379\n","               River       0.98      0.99      0.99       450\n","         Residential       0.98      0.97      0.98       375\n","             SeaLake       0.99      1.00      0.99       406\n","\n","            accuracy                           0.98      4050\n","           macro avg       0.98      0.98      0.98      4050\n","        weighted avg       0.98      0.98      0.98      4050\n","\n","Model exported to ONNX format at /content/drive/MyDrive/Colab Notebooks/Euro/results_pytorch_all/efficientnet_eurosat.onnx\n"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.onnx\n","import torch.optim as optim\n","from torchvision import transforms\n","from torchvision.datasets import EuroSAT\n","from torch.utils.data import DataLoader, random_split\n","from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n","from torchvision.utils import save_image\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import classification_report, confusion_matrix\n","import seaborn as sns\n","import numpy as np\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Install dependencies if missing\n","try:\n","    import onnx\n","except ImportError:\n","    print(\"Installing onnx...\")\n","    os.system(\"pip install onnx\")\n","try:\n","    import torchvision\n","    if torchvision.__version__ < '0.14.0':\n","        print(\"torchvision version too old. Installing latest...\")\n","        os.system(\"pip install torchvision --upgrade\")\n","except ImportError:\n","    print(\"Installing torchvision...\")\n","    os.system(\"pip install torchvision\")\n","\n","# Define directories in Google Colab (adjust if running locally)\n","base_colab_path = r\"/content/drive/MyDrive/Colab Notebooks/Euro\"\n","results_dir = os.path.join(base_colab_path, 'results_pytorch_all')\n","stored_test_images_dir = os.path.join(results_dir, 'stored_test_images')\n","data_dir = os.path.join(base_colab_path, 'data')\n","\n","# Create necessary directories\n","for dir_path in [base_colab_path, results_dir, stored_test_images_dir, data_dir]:\n","    try:\n","        os.makedirs(dir_path, exist_ok=True)\n","        print(f\"Created/Verified directory: {dir_path}\")\n","    except PermissionError:\n","        print(f\"Permission denied for {dir_path}. Check Google Drive permissions.\")\n","        exit(1)\n","\n","# Verify write permissions\n","try:\n","    test_path = os.path.join(data_dir, 'test.txt')\n","    with open(test_path, 'w') as f:\n","        f.write(\"Test\")\n","    os.remove(test_path)\n","    print(f\"Write permission confirmed for {data_dir}\")\n","except PermissionError:\n","    print(f\"Permission denied for {data_dir}. Check Google Drive permissions.\")\n","    exit(1)\n","\n","# Global variables\n","num_epochs = 30\n","batch_size = 64\n","learning_rate = 0.0001  # Changed to 0.0001 for stability\n","random_seed = 42\n","class_names = [\n","    'AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial',\n","    'Pasture', 'PermanentCrop', 'River', 'Residential', 'SeaLake'\n","]\n","\n","def load_data(batch_size=32, val_split=0.15, test_split=0.15, random_seed=42):\n","    transform = transforms.Compose([\n","        transforms.Resize((64, 64)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    # Check if dataset exists\n","    dataset_path = os.path.join(data_dir, 'eurosat')\n","    download_needed = not os.path.exists(dataset_path) or not os.listdir(dataset_path)\n","\n","    if not download_needed:\n","        print(f\"Dataset found at {dataset_path}. Using existing dataset.\")\n","        try:\n","            full_dataset = EuroSAT(root=data_dir, download=False, transform=transform)\n","            print(f\"Loaded dataset with {len(full_dataset)} images\")\n","        except Exception as e:\n","            print(f\"Failed to load existing dataset: {e}\")\n","            download_needed = True\n","\n","    if download_needed:\n","        print(f\"Dataset not found or empty at {dataset_path}. Attempting to download...\")\n","        try:\n","            full_dataset = EuroSAT(root=data_dir, download=True, transform=transform)\n","            print(f\"Dataset downloaded to {dataset_path}\")\n","        except Exception as e:\n","            print(f\"Failed to download EuroSAT: {e}\")\n","            print(\"Please download the dataset manually from https://archive.ics.uci.edu/ml/datasets/EuroSAT\")\n","            print(f\"Extract to {dataset_path} with structure: {dataset_path}/AnnualCrop, {dataset_path}/Forest, etc.\")\n","            exit(1)\n","\n","    # Verify dataset contents\n","    if not os.path.exists(dataset_path) or not os.listdir(dataset_path):\n","        print(f\"Dataset folder {dataset_path} is empty or missing. Please ensure the dataset is correctly downloaded.\")\n","        exit(1)\n","    else:\n","        print(f\"Dataset verified. Contents: {os.listdir(dataset_path)}\")\n","\n","    generator = torch.Generator().manual_seed(random_seed)\n","    n_total = len(full_dataset)\n","    n_val = int(val_split * n_total)\n","    n_test = int(test_split * n_total)\n","    n_train = n_total - n_val - n_test\n","\n","    train_dataset, val_dataset, test_dataset = random_split(\n","        full_dataset, [n_train, n_val, n_test], generator=generator)\n","\n","    unnormalized_transform = transforms.Compose([\n","        transforms.Resize((64, 64)),\n","        transforms.ToTensor()\n","    ])\n","    raw_dataset = EuroSAT(root=data_dir, download=False, transform=unnormalized_transform)\n","\n","    test_indices = getattr(test_dataset, 'indices', None) or test_dataset._indices\n","    for idx in test_indices[:5]:  # Store only the first 5 test images for visualization\n","        img_tensor, label = raw_dataset[idx]\n","        class_name = class_names[label]\n","        class_folder = os.path.join(stored_test_images_dir, class_name)\n","        os.makedirs(class_folder, exist_ok=True)\n","        save_path = os.path.join(class_folder, f\"{idx}.png\")\n","        save_image(img_tensor, save_path)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","    return train_loader, val_loader, test_loader\n","\n","def build_model():\n","    model = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n","    model.classifier[1] = nn.Linear(model.classifier[1].in_features, len(class_names))\n","    return model\n","\n","def visualize_predictions(images, labels, preds, epoch, phase):\n","    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n","    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n","    denorm_images = images * std.to(images.device) + mean.to(images.device)\n","    denorm_images = torch.clamp(denorm_images, 0, 1)\n","    fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n","    for i in range(5):\n","        ax = axes[i]\n","        ax.imshow(denorm_images[i].permute(1, 2, 0).cpu().numpy())\n","        ax.set_title(f\"True: {class_names[labels[i]]}\\nPred: {class_names[preds[i]]}\")\n","        ax.axis('off')\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(results_dir, f'{phase}_predictions_epoch_{epoch+1}.png'))\n","    plt.close()\n","\n","def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device):\n","    model.to(device)\n","    train_losses, val_losses = [], []\n","    best_val_loss = float('inf')\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss = 0\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        if (epoch + 1) % 5 == 0:\n","            model.eval()\n","            with torch.no_grad():\n","                images, labels = next(iter(train_loader))\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                _, preds = torch.max(outputs, 1)\n","                visualize_predictions(images, labels, preds, epoch, phase=\"train\")\n","\n","        val_loss = 0\n","        model.eval()\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","\n","        val_loss_avg = val_loss / len(val_loader)\n","        train_losses.append(train_loss / len(train_loader))\n","        val_losses.append(val_loss_avg)\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n","\n","        # Step the scheduler\n","        scheduler.step(val_loss_avg)\n","        print(f\"Current Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n","\n","        # Save best model\n","        if val_loss_avg < best_val_loss:\n","            best_val_loss = val_loss_avg\n","            torch.save(model.state_dict(), os.path.join(results_dir, 'best_model.pth'))\n","\n","    return train_losses, val_losses\n","\n","def evaluate_model(model, test_loader, device):\n","    model.eval()\n","    model.to(device)\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images = images.to(device)\n","            outputs = model(images)\n","            _, preds = torch.max(outputs, 1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.numpy())\n","\n","        # Visualize predictions on a few test images\n","        images, labels = next(iter(test_loader))\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        _, preds = torch.max(outputs, 1)\n","        visualize_predictions(images, labels, preds, epoch=num_epochs, phase=\"test\")\n","\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(all_labels, all_preds, target_names=class_names))\n","\n","    cm = confusion_matrix(all_labels, all_preds)\n","    plt.figure(figsize=(12, 10))\n","    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.title('Confusion Matrix')\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(results_dir, 'confusion_matrix.png'))\n","    plt.close()\n","\n","def plot_losses(train_losses, val_losses):\n","    plt.figure()\n","    plt.plot(train_losses, label='Train Loss')\n","    plt.plot(val_losses, label='Validation Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.title('Training and Validation Loss')\n","    plt.grid()\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(results_dir, 'loss_curve.png'))\n","    plt.close()\n","\n","def save_model(model):\n","    torch.save(model.state_dict(), os.path.join(results_dir, 'efficientnet_eurosat.pth'))\n","\n","def export_model_to_onnx(model, device):\n","    try:\n","        onnx_path = os.path.join(results_dir, 'efficientnet_eurosat.onnx')\n","        dummy_input = torch.randn(1, 3, 64, 64).to(device)\n","        torch.onnx.export(model, dummy_input, onnx_path, verbose=True)\n","        print(f\"Model exported to ONNX format at {onnx_path}\")\n","    except Exception as e:\n","        print(f\"Failed to export to ONNX: {e}\")\n","        print(\"Continuing without ONNX export.\")\n","\n","if __name__ == '__main__':\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Using device: {device}\")\n","\n","    train_loader, val_loader, test_loader = load_data(batch_size, random_seed=random_seed)\n","    model = build_model()\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n","\n","    train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device)\n","    plot_losses(train_losses, val_losses)\n","    evaluate_model(model, test_loader, device)\n","\n","    save_model(model)\n","    export_model_to_onnx(model, device)"]}]}